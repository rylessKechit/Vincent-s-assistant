"""
Service d'analyse de donn√©es universel
Analyse intelligente de tous types de datasets CSV avec d√©tection automatique des patterns business
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union
from loguru import logger
import re
from collections import Counter
from datetime import datetime
import json

class DataAnalyzer:
    """Analyseur de donn√©es universel - compatible tous domaines business"""
    
    def __init__(self):
        self.ready = True
        
        # Domaines business d√©tectables
        self.business_domains = {
            'rental_car': {
                'keywords': ['branch', 'upsell', 'upgrade', 'proposal', 'downgrade', 'rental', 'car', 'vehicle', 'fleet'],
                'metrics': ['price', 'revenue', 'up', 'down', 'proposal'],
                'entities': ['branch', 'location', 'station']
            },
            'sales': {
                'keywords': ['revenue', 'profit', 'commission', 'target', 'achievement', 'sales', 'deal', 'conversion'],
                'metrics': ['amount', 'total', 'commission', 'target', 'achievement'],
                'entities': ['salesperson', 'agent', 'territory', 'region']
            },
            'hr': {
                'keywords': ['employee', 'department', 'salary', 'performance', 'evaluation', 'hr', 'staff'],
                'metrics': ['salary', 'rating', 'score', 'bonus'],
                'entities': ['employee', 'department', 'manager', 'team']
            },
            'finance': {
                'keywords': ['amount', 'cost', 'price', 'budget', 'expense', 'financial', 'accounting'],
                'metrics': ['amount', 'cost', 'expense', 'budget', 'profit', 'loss'],
                'entities': ['account', 'center', 'department']
            },
            'marketing': {
                'keywords': ['campaign', 'conversion', 'click', 'impression', 'ctr', 'marketing', 'advertising'],
                'metrics': ['clicks', 'impressions', 'conversions', 'ctr', 'cpc', 'cpm'],
                'entities': ['campaign', 'channel', 'source']
            },
            'ecommerce': {
                'keywords': ['product', 'order', 'customer', 'purchase', 'cart', 'checkout', 'shipping'],
                'metrics': ['price', 'quantity', 'total', 'shipping'],
                'entities': ['customer', 'product', 'order', 'category']
            },
            'logistics': {
                'keywords': ['delivery', 'shipping', 'transport', 'warehouse', 'inventory', 'stock'],
                'metrics': ['quantity', 'weight', 'volume', 'time', 'cost'],
                'entities': ['warehouse', 'route', 'vehicle', 'driver']
            }
        }
        
        # Patterns de performance universels
        self.performance_keywords = [
            'revenue', 'profit', 'sales', 'income', 'earnings', 'total', 'amount',
            'price', 'cost', 'value', 'score', 'rating', 'performance', 'efficiency',
            'target', 'goal', 'achievement', 'kpi', 'metric', 'volume', 'quantity'
        ]
        
        # Patterns d'entit√©s universels
        self.entity_keywords = [
            'id', 'name', 'code', 'ref', 'reference', 'number', 'num',
            'branch', 'office', 'store', 'location', 'region', 'territory',
            'agent', 'employee', 'user', 'customer', 'client', 'account',
            'product', 'item', 'service', 'category', 'type', 'group'
        ]
    
    def is_ready(self) -> bool:
        return self.ready
    
    async def analyze_dataframe(self, dataframe_data: Dict[str, Any], metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyse universelle d'un DataFrame
        
        Args:
            dataframe_data: Donn√©es du DataFrame
            metadata: M√©tadonn√©es d'extraction
            
        Returns:
            Dict avec analyse compl√®te
        """
        
        try:
            logger.info("üß† D√©but analyse universelle du DataFrame")
            
            # Reconstituer le DataFrame
            df = self._reconstruct_dataframe(dataframe_data)
            logger.info(f"üìä DataFrame reconstitu√©: {df.shape}")
            
            # 1. Analyse universelle des colonnes
            column_analysis = self._analyze_all_columns_universal(df)
            logger.info(f"üîç Analyse colonnes: {len(column_analysis)} colonnes analys√©es")
            
            # 2. D√©tection automatique du domaine business
            domain_detection = self._detect_business_domain(df, metadata)
            logger.info(f"üéØ Domaine d√©tect√©: {domain_detection.get('primary_domain', 'unknown')}")
            
            # 3. Analyse des relations entre colonnes
            column_relationships = self._analyze_column_relationships(df)
            logger.info(f"üîó Relations analys√©es: {len(column_relationships)} relations")
            
            # 4. Statistiques descriptives universelles
            descriptive_stats = self._generate_descriptive_statistics(df)
            logger.info("üìà Statistiques descriptives g√©n√©r√©es")
            
            # 5. D√©tection des patterns de donn√©es
            data_patterns = self._detect_universal_data_patterns(df)
            logger.info(f"üîç Patterns d√©tect√©s: {len(data_patterns)} types")
            
            # 6. Analyse de la distribution des donn√©es
            distribution_analysis = self._analyze_data_distributions(df)
            logger.info("üìä Distribution des donn√©es analys√©e")
            
            # 7. D√©tection des outliers et anomalies
            anomaly_detection = self._detect_anomalies_universal(df)
            logger.info(f"‚ö†Ô∏è Anomalies: {len(anomaly_detection.get('outliers', []))} d√©tect√©es")
            
            logger.info("‚úÖ Analyse universelle termin√©e avec succ√®s")
            
            return {
                'success': True,
                'column_analysis': column_analysis,
                'domain_detection': domain_detection,
                'column_relationships': column_relationships,
                'descriptive_stats': descriptive_stats,
                'data_patterns': data_patterns,
                'distribution_analysis': distribution_analysis,
                'anomaly_detection': anomaly_detection,
                'analysis_summary': self._generate_analysis_summary(df, domain_detection)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse universelle: {str(e)}")
            return {
                'success': False,
                'error': f"Erreur analyse: {str(e)}"
            }
    
    async def detect_business_patterns(self, dataframe_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        D√©tection sp√©cialis√©e des patterns business
        
        Args:
            dataframe_data: Donn√©es du DataFrame
            
        Returns:
            Dict avec patterns business d√©tect√©s
        """
        
        try:
            logger.info("üéØ D√©but d√©tection patterns business")
            
            df = self._reconstruct_dataframe(dataframe_data)
            
            # 1. D√©tection automatique du domaine
            domain_info = self._detect_business_domain(df, {})
            primary_domain = domain_info.get('primary_domain', 'unknown')
            
            logger.info(f"üè¢ Domaine principal: {primary_domain}")
            
            # 2. Analyse des m√©triques business universelles
            business_metrics = self._analyze_business_metrics_universal(df)
            
            # 3. Analyse des entit√©s business
            entity_analysis = self._analyze_business_entities(df)
            
            # 4. D√©tection des KPIs automatiquement
            kpi_detection = self._detect_kpis_automatic(df)
            
            # 5. Analyse des patterns sp√©cifiques au domaine
            domain_specific_patterns = self._analyze_domain_specific_patterns(df, primary_domain)
            
            # 6. D√©tection des tendances et corr√©lations
            trend_analysis = self._analyze_business_trends(df)
            
            # 7. Identification des top/bottom performers
            performance_ranking = self._rank_performance_universal(df)
            
            logger.info("‚úÖ D√©tection patterns business termin√©e")
            
            return {
                'domain_info': domain_info,
                'business_metrics': business_metrics,
                'entity_analysis': entity_analysis,
                'kpi_detection': kpi_detection,
                'domain_specific_patterns': domain_specific_patterns,
                'trend_analysis': trend_analysis,
                'performance_ranking': performance_ranking,
                'insights': self._generate_business_insights(df, domain_info)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection patterns business: {str(e)}")
            return {
                'success': False,
                'error': f"Erreur patterns business: {str(e)}"
            }
    
    async def generate_recommendations(self, dataframe_data: Dict[str, Any], analysis_result: Dict[str, Any]) -> List[str]:
        """
        G√©n√®re des recommandations bas√©es sur l'analyse - M√âTHODE LEGACY POUR COMPATIBILIT√â
        
        Args:
            dataframe_data: Donn√©es du DataFrame
            analysis_result: R√©sultat de l'analyse pr√©c√©dente
            
        Returns:
            List des recommandations
        """
        
        try:
            df = self._reconstruct_dataframe(dataframe_data)
            
            recommendations = []
            
            # 1. Recommandations bas√©es sur la qualit√© des donn√©es
            missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
            if missing_ratio > 0.1:
                recommendations.append(f"Am√©liorer la qualit√© des donn√©es - {missing_ratio:.1%} de valeurs manquantes d√©tect√©es")
            
            # 2. Recommandations bas√©es sur l'analyse
            if 'domain_detection' in analysis_result:
                domain = analysis_result['domain_detection'].get('primary_domain', 'unknown')
                
                if domain == 'rental_car':
                    recommendations.append("Optimiser les strat√©gies d'upselling en analysant les branches les plus performantes")
                    recommendations.append("Analyser les facteurs influen√ßant les taux de downgrade pour les r√©duire")
                elif domain == 'sales':
                    recommendations.append("Identifier les meilleures pratiques des top performers pour la formation")
                    recommendations.append("Optimiser l'allocation des territoires bas√©e sur les performances")
                elif domain == 'hr':
                    recommendations.append("Analyser les patterns de r√©tention des employ√©s haute performance")
                    recommendations.append("D√©velopper des programmes de formation cibl√©s par d√©partement")
                elif domain == 'finance':
                    recommendations.append("Optimiser l'allocation budg√©taire bas√©e sur l'analyse des co√ªts")
                    recommendations.append("Identifier les centres de co√ªts n√©cessitant plus d'attention")
            
            # 3. Recommandations bas√©es sur les patterns d√©tect√©s
            if 'business_metrics' in analysis_result:
                metrics = analysis_result['business_metrics'].get('identified_metrics', [])
                if len(metrics) > 3:
                    recommendations.append("Cr√©er un dashboard de suivi pour les m√©triques cl√©s identifi√©es")
                
                # Analyser les m√©triques de pourcentage
                percentage_metrics = [m for m in metrics if '%' in m.lower() or 'percent' in m.lower()]
                if percentage_metrics:
                    recommendations.append("√âtablir des benchmarks pour les m√©triques en pourcentage afin de suivre l'√©volution")
            
            # 4. Recommandations bas√©es sur la structure des donn√©es
            numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
            if numeric_cols > 5:
                recommendations.append("Effectuer une analyse de corr√©lation approfondie entre les variables num√©riques")
            
            if len(df) > 1000:
                recommendations.append("Consid√©rer une segmentation des donn√©es pour des insights plus granulaires")
            
            # 5. Recommandations sp√©cifiques aux outliers
            for col in df.select_dtypes(include=[np.number]).columns:
                outliers_count = self._count_outliers(df[col])
                if outliers_count > len(df) * 0.05:  # Plus de 5% d'outliers
                    recommendations.append(f"Investiguer les valeurs aberrantes dans '{col}' ({outliers_count} d√©tect√©es)")
                    break  # Une recommandation suffit pour les outliers
            
            # 6. Recommandations pour l'am√©lioration continue
            if len(df.columns) > 10:
                recommendations.append("Prioriser les variables les plus importantes pour simplifier l'analyse")
            
            # S'assurer qu'on a des recommandations
            if not recommendations:
                recommendations = [
                    "Donn√©es de bonne qualit√© d√©tect√©es - continuer le monitoring r√©gulier",
                    "Mettre en place des alertes automatiques pour d√©tecter les changements de patterns",
                    "Documenter les insights d√©couverts pour r√©f√©rence future"
                ]
            
            # Limiter √† 10 recommandations maximum pour √©viter la surcharge
            return recommendations[:10]
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration recommandations: {str(e)}")
            return [
                "Erreur lors de la g√©n√©ration des recommandations",
                "V√©rifier la qualit√© des donn√©es d'entr√©e",
                "Consulter les logs pour plus de d√©tails"
            ]

    # ... Le reste du code reste exactement identique ...
    # (Je continue avec toutes les autres m√©thodes sans changement)
    
    def _reconstruct_dataframe(self, dataframe_data: Dict[str, Any]) -> pd.DataFrame:
        """Reconstitue un DataFrame depuis les donn√©es JSON"""
        
        if 'data' in dataframe_data and 'columns' in dataframe_data:
            df = pd.DataFrame(dataframe_data['data'])
            if dataframe_data['columns']:
                df.columns = dataframe_data['columns']
        elif 'data' in dataframe_data:
            df = pd.DataFrame(dataframe_data['data'])
        else:
            df = pd.DataFrame(dataframe_data)
        
        return df
    
    def _analyze_all_columns_universal(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse universelle et approfondie de toutes les colonnes"""
        
        analysis = {}
        
        for col in df.columns:
            col_analysis = {
                'basic_stats': self._get_basic_column_stats(df, col),
                'data_type_analysis': self._analyze_column_data_type(df, col),
                'business_relevance': self._assess_business_relevance(col),
                'quality_assessment': self._assess_column_quality(df, col),
                'pattern_detection': self._detect_column_patterns(df, col)
            }
            
            # Analyse sp√©cialis√©e selon le type d√©tect√©
            if col_analysis['data_type_analysis']['is_numeric']:
                col_analysis['numeric_analysis'] = self._analyze_numeric_column(df, col)
            
            if col_analysis['data_type_analysis']['is_categorical']:
                col_analysis['categorical_analysis'] = self._analyze_categorical_column(df, col)
            
            if col_analysis['data_type_analysis']['is_percentage']:
                col_analysis['percentage_analysis'] = self._analyze_percentage_column(df, col)
            
            analysis[col] = col_analysis
        
        return analysis
    
    def _get_basic_column_stats(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Statistiques de base pour une colonne"""
        
        return {
            'count': int(df[col].count()),
            'null_count': int(df[col].isnull().sum()),
            'unique_count': int(df[col].nunique()),
            'duplicate_count': int(df[col].duplicated().sum()),
            'null_percentage': round(df[col].isnull().sum() / len(df) * 100, 2),
            'unique_percentage': round(df[col].nunique() / len(df) * 100, 2)
        }
    
    def _analyze_column_data_type(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse approfondie du type de donn√©es d'une colonne"""
        
        sample = df[col].dropna()
        if len(sample) == 0:
            return {'is_empty': True}
        
        sample_str = sample.astype(str)
        
        analysis = {
            'pandas_dtype': str(df[col].dtype),
            'is_numeric': False,
            'is_categorical': False,
            'is_datetime': False,
            'is_percentage': False,
            'is_currency': False,
            'is_identifier': False,
            'special_patterns': []
        }
        
        # D√©tection num√©rique
        try:
            pd.to_numeric(sample, errors='coerce')
            numeric_ratio = pd.to_numeric(sample, errors='coerce').notna().sum() / len(sample)
            if numeric_ratio > 0.8:
                analysis['is_numeric'] = True
        except:
            pass
        
        # D√©tection pourcentages
        if sample_str.str.contains('%').sum() / len(sample) > 0.5:
            analysis['is_percentage'] = True
            analysis['special_patterns'].append('percentage')
        
        # D√©tection monnaie
        currency_pattern = r'[$‚Ç¨¬£¬•]|USD|EUR|CHF|GBP'
        if sample_str.str.contains(currency_pattern, case=False).sum() / len(sample) > 0.3:
            analysis['is_currency'] = True
            analysis['special_patterns'].append('currency')
        
        # D√©tection cat√©gorique
        unique_ratio = sample.nunique() / len(sample)
        if unique_ratio < 0.5 and sample.nunique() < 50:
            analysis['is_categorical'] = True
        
        # D√©tection identifiant
        if unique_ratio > 0.9:
            analysis['is_identifier'] = True
            analysis['special_patterns'].append('identifier')
        
        # D√©tection codes/r√©f√©rences
        if sample_str.str.contains(r'\d{3,}|\w+\d+|\d+\w+').sum() / len(sample) > 0.7:
            analysis['special_patterns'].append('code_reference')
        
        return analysis
    
    def _assess_business_relevance(self, column_name: str) -> Dict[str, Any]:
        """√âvalue la pertinence business d'une colonne"""
        
        col_lower = column_name.lower()
        
        relevance = {
            'is_performance_metric': False,
            'is_entity_identifier': False,
            'is_temporal': False,
            'business_category': 'other',
            'importance_score': 0
        }
        
        # Performance metrics
        for keyword in self.performance_keywords:
            if keyword in col_lower:
                relevance['is_performance_metric'] = True
                relevance['business_category'] = 'performance'
                relevance['importance_score'] += 3
                break
        
        # Entity identifiers
        for keyword in self.entity_keywords:
            if keyword in col_lower:
                relevance['is_entity_identifier'] = True
                relevance['business_category'] = 'entity'
                relevance['importance_score'] += 2
                break
        
        # Temporal indicators
        temporal_keywords = ['date', 'time', 'year', 'month', 'day', 'period']
        for keyword in temporal_keywords:
            if keyword in col_lower:
                relevance['is_temporal'] = True
                relevance['business_category'] = 'temporal'
                relevance['importance_score'] += 2
                break
        
        # Pourcentages et ratios
        if any(keyword in col_lower for keyword in ['%', 'percent', 'ratio', 'rate']):
            relevance['importance_score'] += 2
        
        return relevance
    
    def _assess_column_quality(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """√âvaluation de la qualit√© d'une colonne"""
        
        total_rows = len(df)
        null_count = df[col].isnull().sum()
        completeness = 1 - (null_count / total_rows)
        
        # D√©tection des valeurs aberrantes dans les strings
        if df[col].dtype == 'object':
            sample = df[col].dropna().astype(str)
            if len(sample) > 0:
                avg_length = sample.str.len().mean()
                length_std = sample.str.len().std()
                unusual_lengths = ((sample.str.len() - avg_length).abs() > 3 * length_std).sum()
            else:
                unusual_lengths = 0
        else:
            unusual_lengths = 0
        
        quality_score = completeness * 100
        if unusual_lengths / total_rows > 0.1:  # Plus de 10% de valeurs inhabituelles
            quality_score -= 10
        
        return {
            'completeness': round(completeness * 100, 2),
            'quality_score': round(quality_score, 2),
            'issues': self._identify_column_issues(df, col),
            'recommendations': self._generate_column_recommendations(df, col)
        }
    
    def _identify_column_issues(self, df: pd.DataFrame, col: str) -> List[str]:
        """Identifie les probl√®mes sp√©cifiques d'une colonne"""
        
        issues = []
        null_percentage = df[col].isnull().sum() / len(df) * 100
        
        if null_percentage > 50:
            issues.append(f"Trop de valeurs manquantes ({null_percentage:.1f}%)")
        elif null_percentage > 20:
            issues.append(f"Valeurs manquantes significatives ({null_percentage:.1f}%)")
        
        if df[col].dtype == 'object':
            sample = df[col].dropna().astype(str)
            if len(sample) > 0:
                # D√©tection de formats inconsistants
                length_variance = sample.str.len().var()
                if length_variance > 100:  # Grande variance dans les longueurs
                    issues.append("Formats de donn√©es inconsistants d√©tect√©s")
                
                # D√©tection de caract√®res sp√©ciaux probl√©matiques
                special_chars = sample.str.contains(r'[^\w\s\-\.\%\$\‚Ç¨\¬£]', regex=True).sum()
                if special_chars / len(sample) > 0.1:
                    issues.append("Caract√®res sp√©ciaux inhabituels d√©tect√©s")
        
        return issues
    
    def _generate_column_recommendations(self, df: pd.DataFrame, col: str) -> List[str]:
        """G√©n√®re des recommandations pour am√©liorer une colonne"""
        
        recommendations = []
        null_percentage = df[col].isnull().sum() / len(df) * 100
        
        if null_percentage > 20:
            recommendations.append(f"Investiguer les causes des valeurs manquantes dans '{col}'")
        
        if df[col].dtype == 'object':
            sample = df[col].dropna().astype(str)
            if len(sample) > 0:
                # V√©rifier si peut √™tre converti en num√©rique
                numeric_convertible = 0
                for val in sample.head(20):
                    try:
                        float(str(val).replace('%', '').replace(',', '').replace('‚Ç¨', '').replace('$', ''))
                        numeric_convertible += 1
                    except:
                        pass
                
                if numeric_convertible / min(20, len(sample)) > 0.8:
                    recommendations.append(f"Colonne '{col}' pourrait √™tre convertie en num√©rique")
        
        if not recommendations:
            recommendations.append("Aucune am√©lioration imm√©diate sugg√©r√©e")
        
        return recommendations
    
    def _detect_column_patterns(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """D√©tecte les patterns sp√©cifiques dans une colonne"""
        
        patterns = {
            'has_patterns': False,
            'detected_patterns': []
        }
        
        if df[col].dtype == 'object':
            sample = df[col].dropna().astype(str)
            if len(sample) == 0:
                return patterns
            
            # Pattern codes (ex: 8139, ABC123)
            if sample.str.contains(r'^\d{3,6}$').sum() / len(sample) > 0.7:
                patterns['detected_patterns'].append('numeric_codes')
                patterns['has_patterns'] = True
            
            # Pattern codes alphanum√©riques
            if sample.str.contains(r'^[A-Z0-9]{3,}$').sum() / len(sample) > 0.5:
                patterns['detected_patterns'].append('alphanumeric_codes')
                patterns['has_patterns'] = True
            
            # Pattern pourcentages
            if sample.str.contains(r'\d+\.?\d*%').sum() / len(sample) > 0.5:
                patterns['detected_patterns'].append('percentages')
                patterns['has_patterns'] = True
            
            # Pattern noms de lieux (contient des parenth√®ses, tirets)
            if sample.str.contains(r'[A-Za-z]+\s*[\(\-]').sum() / len(sample) > 0.5:
                patterns['detected_patterns'].append('location_names')
                patterns['has_patterns'] = True
        
        return patterns
    
    # Toutes les autres m√©thodes restent identiques... (continuit√© du code original)
    # Note: Pour √©conomiser l'espace, je r√©f√©rence le fait que toutes les autres m√©thodes 
    # du analyzer_PARFAIT.py restent exactement les m√™mes
    
    def _analyze_numeric_column(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse approfondie d'une colonne num√©rique"""
        
        numeric_data = pd.to_numeric(df[col], errors='coerce')
        
        if numeric_data.count() == 0:
            return {'error': 'Pas de donn√©es num√©riques valides'}
        
        return {
            'mean': float(numeric_data.mean()),
            'median': float(numeric_data.median()),
            'std': float(numeric_data.std()),
            'min': float(numeric_data.min()),
            'max': float(numeric_data.max()),
            'q25': float(numeric_data.quantile(0.25)),
            'q75': float(numeric_data.quantile(0.75)),
            'skewness': float(numeric_data.skew()),
            'kurtosis': float(numeric_data.kurtosis()),
            'outliers_count': self._count_outliers(numeric_data),
            'distribution_type': self._classify_distribution(numeric_data)
        }
    
    def _analyze_categorical_column(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse approfondie d'une colonne cat√©gorielle"""
        
        value_counts = df[col].value_counts()
        
        return {
            'unique_values': int(df[col].nunique()),
            'most_frequent': value_counts.index[0] if len(value_counts) > 0 else None,
            'most_frequent_count': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
            'least_frequent': value_counts.index[-1] if len(value_counts) > 0 else None,
            'least_frequent_count': int(value_counts.iloc[-1]) if len(value_counts) > 0 else 0,
            'value_distribution': value_counts.head(10).to_dict(),
            'entropy': self._calculate_entropy(value_counts),
            'concentration_ratio': self._calculate_concentration_ratio(value_counts)
        }
    
    def _analyze_percentage_column(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse sp√©cialis√©e pour les colonnes de pourcentages"""
        
        # Extraire les valeurs num√©riques des pourcentages
        sample = df[col].dropna().astype(str)
        numeric_values = []
        
        for val in sample:
            try:
                # Nettoyer et convertir
                cleaned = val.replace('%', '').replace(',', '').replace(' ', '')
                numeric_values.append(float(cleaned))
            except:
                pass
        
        if not numeric_values:
            return {'error': 'Impossible d\'extraire les valeurs num√©riques'}
        
        numeric_series = pd.Series(numeric_values)
        
        return {
            'average_percentage': round(numeric_series.mean(), 2),
            'median_percentage': round(numeric_series.median(), 2),
            'min_percentage': round(numeric_series.min(), 2),
            'max_percentage': round(numeric_series.max(), 2),
            'std_percentage': round(numeric_series.std(), 2),
            'negative_count': int((numeric_series < 0).sum()),
            'zero_count': int((numeric_series == 0).sum()),
            'above_100_count': int((numeric_series > 100).sum()),
            'quartiles': {
                'q25': round(numeric_series.quantile(0.25), 2),
                'q50': round(numeric_series.quantile(0.50), 2),
                'q75': round(numeric_series.quantile(0.75), 2)
            }
        }
    
    def _detect_business_domain(self, df: pd.DataFrame, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """D√©tection automatique du domaine business"""
        
        # Analyser les noms de colonnes
        all_columns_text = ' '.join(df.columns).lower()
        
        domain_scores = {}
        
        for domain, config in self.business_domains.items():
            score = 0
            matched_keywords = []
            
            # Compter les correspondances de keywords
            for keyword in config['keywords']:
                if keyword in all_columns_text:
                    score += 2
                    matched_keywords.append(keyword)
            
            # Compter les correspondances de m√©triques
            for metric in config['metrics']:
                if metric in all_columns_text:
                    score += 3  # Poids plus √©lev√© pour les m√©triques
                    matched_keywords.append(metric)
            
            # Compter les correspondances d'entit√©s
            for entity in config['entities']:
                if entity in all_columns_text:
                    score += 2
                    matched_keywords.append(entity)
            
            if score > 0:
                domain_scores[domain] = {
                    'score': score,
                    'matched_keywords': matched_keywords,
                    'confidence': min(score / 10, 1.0)  # Normaliser √† 1.0 max
                }
        
        # D√©terminer le domaine principal
        if domain_scores:
            primary_domain = max(domain_scores.items(), key=lambda x: x[1]['score'])
            primary_domain_name = primary_domain[0]
            primary_domain_info = primary_domain[1]
        else:
            primary_domain_name = 'unknown'
            primary_domain_info = {'score': 0, 'matched_keywords': [], 'confidence': 0}
        
        return {
            'primary_domain': primary_domain_name,
            'confidence': primary_domain_info['confidence'],
            'matched_keywords': primary_domain_info['matched_keywords'],
            'all_domain_scores': domain_scores,
            'domain_explanation': self._explain_domain_detection(primary_domain_name, primary_domain_info)
        }
    
    def _explain_domain_detection(self, domain: str, info: Dict[str, Any]) -> str:
        """Explique pourquoi ce domaine a √©t√© d√©tect√©"""
        
        if domain == 'unknown':
            return "Aucun domaine business sp√©cifique d√©tect√© dans les colonnes"
        
        keywords = ', '.join(info['matched_keywords'][:5])  # Limiter √† 5 mots-cl√©s
        confidence_pct = int(info['confidence'] * 100)
        
        return f"Domaine '{domain}' d√©tect√© avec {confidence_pct}% de confiance bas√© sur: {keywords}"
    
    def _analyze_business_metrics_universal(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse universelle des m√©triques business"""
        
        metrics = {}
        
        # Identifier les colonnes de performance potentielles
        for col in df.columns:
            col_lower = col.lower()
            
            # V√©rifier si c'est une m√©trique de performance
            is_performance_metric = any(keyword in col_lower for keyword in self.performance_keywords)
            
            if is_performance_metric:
                # Analyser selon le type de donn√©es
                if df[col].dtype in ['float64', 'int64']:
                    metrics[col] = self._analyze_numeric_performance_metric(df, col)
                elif df[col].dtype == 'object':
                    # V√©rifier si ce sont des pourcentages ou montants
                    sample = df[col].dropna().astype(str)
                    if len(sample) > 0:
                        if sample.str.contains('%').any():
                            metrics[col] = self._analyze_percentage_metric(df, col)
                        elif sample.str.contains(r'[\$‚Ç¨¬£¬•]').any():
                            metrics[col] = self._analyze_currency_metric(df, col)
        
        return {
            'identified_metrics': list(metrics.keys()),
            'metrics_analysis': metrics,
            'summary': self._summarize_metrics_analysis(metrics)
        }
    
    def _analyze_numeric_performance_metric(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse d'une m√©trique de performance num√©rique"""
        
        data = df[col].dropna()
        if len(data) == 0:
            return {'error': 'Pas de donn√©es'}
        
        return {
            'total': float(data.sum()),
            'average': float(data.mean()),
            'median': float(data.median()),
            'min': float(data.min()),
            'max': float(data.max()),
            'std': float(data.std()),
            'best_performer_index': int(data.idxmax()),
            'worst_performer_index': int(data.idxmin()),
            'top_10_percent_threshold': float(data.quantile(0.9)),
            'bottom_10_percent_threshold': float(data.quantile(0.1))
        }
    
    def _analyze_percentage_metric(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse d'une m√©trique sous forme de pourcentage"""
        
        # Extraire les valeurs num√©riques
        sample = df[col].dropna().astype(str)
        numeric_values = []
        
        for val in sample:
            try:
                cleaned = val.replace('%', '').replace(',', '').replace(' ', '')
                numeric_values.append(float(cleaned))
            except:
                pass
        
        if not numeric_values:
            return {'error': 'Impossible d\'extraire les pourcentages'}
        
        data = pd.Series(numeric_values)
        
        return {
            'average_percentage': round(data.mean(), 2),
            'median_percentage': round(data.median(), 2),
            'best_percentage': round(data.max(), 2),
            'worst_percentage': round(data.min(), 2),
            'std_percentage': round(data.std(), 2),
            'above_average_count': int((data > data.mean()).sum()),
            'negative_values_count': int((data < 0).sum()),
            'excellent_threshold': round(data.quantile(0.9), 2),
            'poor_threshold': round(data.quantile(0.1), 2)
        }
    
    def _analyze_currency_metric(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse d'une m√©trique mon√©taire"""
        
        sample = df[col].dropna().astype(str)
        numeric_values = []
        
        for val in sample:
            try:
                # Nettoyer les symboles mon√©taires et convertir
                cleaned = re.sub(r'[$‚Ç¨¬£¬•,]', '', val).replace(' ', '')
                numeric_values.append(float(cleaned))
            except:
                pass
        
        if not numeric_values:
            return {'error': 'Impossible d\'extraire les montants'}
        
        data = pd.Series(numeric_values)
        
        return {
            'total_amount': round(data.sum(), 2),
            'average_amount': round(data.mean(), 2),
            'median_amount': round(data.median(), 2),
            'highest_amount': round(data.max(), 2),
            'lowest_amount': round(data.min(), 2),
            'high_value_threshold': round(data.quantile(0.8), 2),
            'low_value_threshold': round(data.quantile(0.2), 2)
        }
    
    def _analyze_business_entities(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse des entit√©s business (branches, agents, etc.)"""
        
        entities = {}
        
        for col in df.columns:
            col_lower = col.lower()
            
            # V√©rifier si c'est une colonne d'entit√©
            is_entity = any(keyword in col_lower for keyword in self.entity_keywords)
            
            if is_entity or df[col].nunique() / len(df) < 0.8:  # Crit√®re de cardinalit√©
                entity_analysis = {
                    'unique_count': int(df[col].nunique()),
                    'total_count': int(df[col].count()),
                    'null_count': int(df[col].isnull().sum()),
                    'top_entities': df[col].value_counts().head(10).to_dict(),
                    'entity_distribution': self._analyze_entity_distribution(df[col])
                }
                
                entities[col] = entity_analysis
        
        return entities
    
    def _analyze_entity_distribution(self, series: pd.Series) -> Dict[str, Any]:
        """Analyse la distribution d'une colonne d'entit√©s"""
        
        value_counts = series.value_counts()
        
        return {
            'most_frequent_entity': value_counts.index[0] if len(value_counts) > 0 else None,
            'most_frequent_count': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
            'single_occurrence_entities': int((value_counts == 1).sum()),
            'distribution_evenness': self._calculate_distribution_evenness(value_counts)
        }
    
    def _calculate_distribution_evenness(self, value_counts: pd.Series) -> float:
        """Calcule l'uniformit√© de la distribution (0 = tr√®s in√©gale, 1 = parfaitement √©gale)"""
        if len(value_counts) <= 1:
            return 1.0
        
        # Utiliser l'entropie normalis√©e
        total = value_counts.sum()
        probabilities = value_counts / total
        entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
        max_entropy = np.log2(len(value_counts))
        
        return entropy / max_entropy if max_entropy > 0 else 0
    
    def _detect_kpis_automatic(self, df: pd.DataFrame) -> Dict[str, Any]:
        """D√©tection automatique des KPIs dans le dataset"""
        
        potential_kpis = {}
        
        for col in df.columns:
            col_lower = col.lower()
            
            # Score d'importance pour d√©terminer si c'est un KPI
            kpi_score = 0
            kpi_indicators = []
            
            # Mots-cl√©s KPI
            kpi_keywords = ['target', 'goal', 'objective', 'kpi', 'metric', 'performance', 'achievement']
            for keyword in kpi_keywords:
                if keyword in col_lower:
                    kpi_score += 3
                    kpi_indicators.append(f"KPI keyword: {keyword}")
            
            # Mots-cl√©s performance
            for keyword in self.performance_keywords:
                if keyword in col_lower:
                    kpi_score += 2
                    kpi_indicators.append(f"Performance keyword: {keyword}")
            
            # Pourcentages (souvent des KPIs)
            if '%' in col or 'percent' in col_lower or 'ratio' in col_lower:
                kpi_score += 2
                kpi_indicators.append("Percentage/ratio indicator")
            
            # Donn√©es num√©riques avec variance significative
            if df[col].dtype in ['float64', 'int64']:
                if df[col].std() > 0 and df[col].count() > 0:
                    cv = df[col].std() / df[col].mean()  # Coefficient de variation
                    if cv > 0.1:  # Variance significative
                        kpi_score += 1
                        kpi_indicators.append("Significant variance in numeric data")
            
            if kpi_score >= 2:  # Seuil pour consid√©rer comme KPI potentiel
                potential_kpis[col] = {
                    'kpi_score': kpi_score,
                    'indicators': kpi_indicators,
                    'confidence': min(kpi_score / 6, 1.0),
                    'analysis': self._analyze_kpi_performance(df, col)
                }
        
        return {
            'detected_kpis': list(potential_kpis.keys()),
            'kpi_analysis': potential_kpis,
            'summary': f"{len(potential_kpis)} KPIs potentiels d√©tect√©s"
        }
    
    def _analyze_kpi_performance(self, df: pd.DataFrame, col: str) -> Dict[str, Any]:
        """Analyse la performance d'un KPI"""
        
        if df[col].dtype in ['float64', 'int64']:
            data = df[col].dropna()
            if len(data) == 0:
                return {'error': 'Pas de donn√©es'}
            
            return {
                'current_avg': round(data.mean(), 2),
                'target_benchmark': round(data.quantile(0.75), 2),  # 75e percentile comme benchmark
                'performance_spread': round(data.max() - data.min(), 2),
                'improvement_potential': round(data.quantile(0.9) - data.mean(), 2)
            }
        
        # Pour les donn√©es textuelles (pourcentages, etc.)
        elif df[col].dtype == 'object':
            sample = df[col].dropna().astype(str)
            if sample.str.contains('%').any():
                # Traiter comme pourcentage
                numeric_values = []
                for val in sample:
                    try:
                        cleaned = val.replace('%', '').replace(',', '')
                        numeric_values.append(float(cleaned))
                    except:
                        pass
                
                if numeric_values:
                    data = pd.Series(numeric_values)
                    return {
                        'current_avg': round(data.mean(), 2),
                        'target_benchmark': round(data.quantile(0.75), 2),
                        'best_performance': round(data.max(), 2),
                        'worst_performance': round(data.min(), 2)
                    }
        
        return {'error': 'Type de donn√©es non support√© pour l\'analyse KPI'}
    
    def _analyze_domain_specific_patterns(self, df: pd.DataFrame, domain: str) -> Dict[str, Any]:
        """Analyse des patterns sp√©cifiques au domaine d√©tect√©"""
        
        if domain == 'unknown':
            return {'message': 'Aucun pattern sp√©cifique - domaine non identifi√©'}
        
        domain_patterns = {}
        
        if domain == 'rental_car':
            domain_patterns = self._analyze_rental_car_patterns(df)
        elif domain == 'sales':
            domain_patterns = self._analyze_sales_patterns(df)
        elif domain == 'hr':
            domain_patterns = self._analyze_hr_patterns(df)
        elif domain == 'finance':
            domain_patterns = self._analyze_finance_patterns(df)
        elif domain == 'marketing':
            domain_patterns = self._analyze_marketing_patterns(df)
        
        return domain_patterns
    
    def _analyze_rental_car_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Patterns sp√©cifiques au secteur location de voitures"""
        
        patterns = {}
        
        # Chercher les colonnes sp√©cifiques au rental car
        branch_col = self._find_column_containing(df, ['branch', 'location', 'station'])
        upsell_cols = self._find_columns_containing(df, ['upsell', 'upgrade', 'up'])
        price_cols = self._find_columns_containing(df, ['price', 'cost', 'amount'])
        
        if branch_col:
            patterns['branch_analysis'] = {
                'total_branches': int(df[branch_col].nunique()),
                'top_performing_branches': df[branch_col].value_counts().head(5).to_dict()
            }
        
        if upsell_cols:
            patterns['upsell_analysis'] = {}
            for col in upsell_cols:
                if df[col].dtype == 'object' and df[col].astype(str).str.contains('%').any():
                    patterns['upsell_analysis'][col] = self._analyze_percentage_metric(df, col)
        
        if price_cols:
            patterns['pricing_analysis'] = {}
            for col in price_cols:
                if df[col].dtype in ['float64', 'int64']:
                    patterns['pricing_analysis'][col] = self._analyze_numeric_performance_metric(df, col)
        
        return patterns
    
    def _analyze_sales_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Patterns sp√©cifiques au secteur ventes"""
        
        patterns = {}
        
        revenue_cols = self._find_columns_containing(df, ['revenue', 'sales', 'amount'])
        agent_col = self._find_column_containing(df, ['agent', 'salesperson', 'rep'])
        
        if revenue_cols:
            patterns['revenue_analysis'] = {}
            for col in revenue_cols:
                if df[col].dtype in ['float64', 'int64']:
                    patterns['revenue_analysis'][col] = self._analyze_numeric_performance_metric(df, col)
        
        if agent_col:
            patterns['agent_performance'] = self._analyze_agent_performance(df, agent_col, revenue_cols)
        
        return patterns
    
    def _analyze_hr_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Patterns sp√©cifiques aux RH"""
        
        patterns = {}
        
        employee_col = self._find_column_containing(df, ['employee', 'staff', 'worker'])
        dept_col = self._find_column_containing(df, ['department', 'dept', 'division'])
        salary_cols = self._find_columns_containing(df, ['salary', 'pay', 'compensation'])
        
        if employee_col:
            patterns['workforce_analysis'] = {
                'total_employees': int(df[employee_col].nunique())
            }
        
        if dept_col:
            patterns['department_analysis'] = {
                'departments': df[dept_col].value_counts().to_dict()
            }
        
        return patterns
    
    def _analyze_finance_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Patterns sp√©cifiques √† la finance"""
        
        patterns = {}
        
        amount_cols = self._find_columns_containing(df, ['amount', 'cost', 'expense', 'budget'])
        account_col = self._find_column_containing(df, ['account', 'center', 'department'])
        
        if amount_cols:
            patterns['financial_metrics'] = {}
            for col in amount_cols:
                if df[col].dtype in ['float64', 'int64']:
                    patterns['financial_metrics'][col] = self._analyze_numeric_performance_metric(df, col)
        
        return patterns
    
    def _analyze_marketing_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Patterns sp√©cifiques au marketing"""
        
        patterns = {}
        
        campaign_col = self._find_column_containing(df, ['campaign', 'channel', 'source'])
        metric_cols = self._find_columns_containing(df, ['clicks', 'impressions', 'conversions', 'ctr'])
        
        if campaign_col:
            patterns['campaign_analysis'] = {
                'total_campaigns': int(df[campaign_col].nunique())
            }
        
        if metric_cols:
            patterns['marketing_metrics'] = {}
            for col in metric_cols:
                if df[col].dtype in ['float64', 'int64']:
                    patterns['marketing_metrics'][col] = self._analyze_numeric_performance_metric(df, col)
        
        return patterns
    
    def _find_column_containing(self, df: pd.DataFrame, keywords: List[str]) -> Optional[str]:
        """Trouve la premi√®re colonne contenant un des mots-cl√©s"""
        
        for col in df.columns:
            col_lower = col.lower()
            for keyword in keywords:
                if keyword in col_lower:
                    return col
        return None
    
    def _find_columns_containing(self, df: pd.DataFrame, keywords: List[str]) -> List[str]:
        """Trouve toutes les colonnes contenant un des mots-cl√©s"""
        
        matching_cols = []
        for col in df.columns:
            col_lower = col.lower()
            for keyword in keywords:
                if keyword in col_lower:
                    matching_cols.append(col)
                    break
        return matching_cols
    
    def _analyze_agent_performance(self, df: pd.DataFrame, agent_col: str, metric_cols: List[str]) -> Dict[str, Any]:
        """Analyse la performance des agents/vendeurs"""
        
        if not metric_cols:
            return {'error': 'Aucune m√©trique de performance trouv√©e'}
        
        # Utiliser la premi√®re m√©trique num√©rique trouv√©e
        performance_col = None
        for col in metric_cols:
            if df[col].dtype in ['float64', 'int64']:
                performance_col = col
                break
        
        if not performance_col:
            return {'error': 'Aucune m√©trique num√©rique trouv√©e'}
        
        agent_performance = df.groupby(agent_col)[performance_col].agg(['sum', 'mean', 'count']).reset_index()
        agent_performance = agent_performance.sort_values('sum', ascending=False)
        
        return {
            'performance_metric': performance_col,
            'top_performers': agent_performance.head(5).to_dict('records'),
            'bottom_performers': agent_performance.tail(5).to_dict('records'),
            'total_agents': int(len(agent_performance))
        }
    
    def _analyze_business_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse des tendances business"""
        
        trends = {}
        
        # Chercher des colonnes temporelles
        date_cols = []
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]' or 'date' in col.lower() or 'time' in col.lower():
                date_cols.append(col)
        
        if date_cols:
            trends['temporal_analysis'] = 'Colonnes temporelles d√©tect√©es - analyse chronologique possible'
        
        # Analyser les corr√©lations entre m√©triques num√©riques
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if len(numeric_cols) >= 2:
            correlation_matrix = df[numeric_cols].corr()
            # Trouver les corr√©lations les plus fortes
            correlations = []
            for i, col1 in enumerate(numeric_cols):
                for j, col2 in enumerate(numeric_cols[i+1:], i+1):
                    corr_value = correlation_matrix.loc[col1, col2]
                    if abs(corr_value) > 0.5:  # Corr√©lation significative
                        correlations.append({
                            'column1': col1,
                            'column2': col2,
                            'correlation': round(corr_value, 3),
                            'strength': 'Strong' if abs(corr_value) > 0.7 else 'Moderate'
                        })
            
            trends['correlations'] = sorted(correlations, key=lambda x: abs(x['correlation']), reverse=True)
        
        return trends
    
    def _rank_performance_universal(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Classement de performance universel"""
        
        rankings = {}
        
        # Identifier la premi√®re colonne d'entit√©
        entity_col = None
        for col in df.columns:
            if df[col].nunique() / len(df) < 0.8 and df[col].nunique() > 1:
                entity_col = col
                break
        
        if not entity_col:
            return {'error': 'Aucune colonne d\'entit√© appropri√©e trouv√©e'}
        
        # Identifier les m√©triques de performance
        performance_cols = []
        for col in df.columns:
            if col != entity_col and df[col].dtype in ['float64', 'int64']:
                performance_cols.append(col)
        
        if not performance_cols:
            return {'error': 'Aucune m√©trique de performance num√©rique trouv√©e'}
        
        # Cr√©er les rankings pour chaque m√©trique
        for metric_col in performance_cols[:3]:  # Limiter √† 3 m√©triques pour √©viter la surcharge
            try:
                grouped = df.groupby(entity_col)[metric_col].agg(['sum', 'mean', 'count']).reset_index()
                grouped = grouped.sort_values('sum', ascending=False)
                
                rankings[f'{metric_col}_ranking'] = {
                    'top_5': grouped.head(5).to_dict('records'),
                    'bottom_5': grouped.tail(5).to_dict('records'),
                    'metric_column': metric_col,
                    'entity_column': entity_col
                }
            except Exception as e:
                logger.warning(f"Erreur ranking pour {metric_col}: {e}")
        
        return rankings
    
    def _analyze_column_relationships(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse des relations entre colonnes"""
        
        relationships = {}
        
        # Analyser les relations entre colonnes num√©riques
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(numeric_cols) >= 2:
            correlation_analysis = self._detailed_correlation_analysis(df[numeric_cols])
            relationships['numeric_correlations'] = correlation_analysis
        
        # Analyser les relations entre colonnes cat√©gorielles et num√©riques
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        
        if categorical_cols and numeric_cols:
            categorical_numeric_relations = self._analyze_categorical_numeric_relations(df, categorical_cols, numeric_cols)
            relationships['categorical_numeric_relations'] = categorical_numeric_relations
        
        return relationships
    
    def _detailed_correlation_analysis(self, numeric_df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse d√©taill√©e des corr√©lations"""
        
        correlation_matrix = numeric_df.corr()
        
        strong_correlations = []
        moderate_correlations = []
        
        cols = correlation_matrix.columns
        for i, col1 in enumerate(cols):
            for j, col2 in enumerate(cols[i+1:], i+1):
                corr_value = correlation_matrix.loc[col1, col2]
                
                if not np.isnan(corr_value):
                    if abs(corr_value) > 0.7:
                        strong_correlations.append({
                            'column1': col1,
                            'column2': col2,
                            'correlation': round(corr_value, 3),
                            'interpretation': 'Very strong positive' if corr_value > 0.7 else 'Very strong negative'
                        })
                    elif abs(corr_value) > 0.5:
                        moderate_correlations.append({
                            'column1': col1,
                            'column2': col2,
                            'correlation': round(corr_value, 3),
                            'interpretation': 'Moderate positive' if corr_value > 0.5 else 'Moderate negative'
                        })
        
        return {
            'strong_correlations': strong_correlations,
            'moderate_correlations': moderate_correlations,
            'correlation_matrix_summary': {
                'highest_correlation': float(correlation_matrix.abs().max().max()),
                'average_correlation': float(correlation_matrix.abs().mean().mean())
            }
        }
    
    def _analyze_categorical_numeric_relations(self, df: pd.DataFrame, cat_cols: List[str], num_cols: List[str]) -> Dict[str, Any]:
        """Analyse relations entre colonnes cat√©gorielles et num√©riques"""
        
        relations = []
        
        for cat_col in cat_cols[:3]:  # Limiter pour performance
            if df[cat_col].nunique() < 20:  # √âviter les colonnes avec trop de cat√©gories
                for num_col in num_cols[:3]:  # Limiter pour performance
                    try:
                        grouped = df.groupby(cat_col)[num_col].agg(['mean', 'std', 'count'])
                        variance_between = grouped['mean'].var()
                        variance_within = grouped['std'].mean()
                        
                        if variance_between > 0:
                            relations.append({
                                'categorical_column': cat_col,
                                'numeric_column': num_col,
                                'variance_ratio': round(variance_between / variance_within, 3) if variance_within > 0 else 0,
                                'categories_count': int(df[cat_col].nunique()),
                                'relationship_strength': 'Strong' if variance_between / variance_within > 2 else 'Weak' if variance_within > 0 else 'Unknown'
                            })
                    except Exception:
                        continue
        
        return sorted(relations, key=lambda x: x.get('variance_ratio', 0), reverse=True)
    
    def _generate_descriptive_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """G√©n√©ration de statistiques descriptives universelles"""
        
        stats = {
            'dataset_overview': {
                'total_rows': int(len(df)),
                'total_columns': int(len(df.columns)),
                'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)
            },
            'data_types': {
                'numeric_columns': int(len(df.select_dtypes(include=[np.number]).columns)),
                'text_columns': int(len(df.select_dtypes(include=['object']).columns)),
                'datetime_columns': int(len(df.select_dtypes(include=['datetime64']).columns))
            },
            'missing_data': {
                'total_missing_cells': int(df.isnull().sum().sum()),
                'columns_with_missing': int((df.isnull().sum() > 0).sum()),
                'worst_column': df.isnull().sum().idxmax() if df.isnull().sum().sum() > 0 else None,
                'worst_column_missing_pct': round(df.isnull().sum().max() / len(df) * 100, 2) if df.isnull().sum().sum() > 0 else 0
            }
        }
        
        # Statistiques pour colonnes num√©riques
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            numeric_stats = df[numeric_cols].describe()
            stats['numeric_summary'] = {
                'columns': list(numeric_cols),
                'summary_stats': numeric_stats.round(2).to_dict()
            }
        
        return stats
    
    def _detect_universal_data_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """D√©tection de patterns universels dans les donn√©es"""
        
        patterns = {
            'detected_patterns': [],
            'pattern_details': {}
        }
        
        # Pattern 1: Colonnes avec beaucoup de valeurs uniques (identifiants)
        for col in df.columns:
            uniqueness = df[col].nunique() / len(df)
            if uniqueness > 0.95:
                patterns['detected_patterns'].append(f'{col}: Likely identifier column')
                patterns['pattern_details'][f'{col}_identifier'] = {
                    'uniqueness_ratio': round(uniqueness, 3),
                    'pattern_type': 'identifier'
                }
        
        # Pattern 2: Colonnes avec peu de valeurs uniques (cat√©gories)
        for col in df.columns:
            if df[col].nunique() < 10 and len(df) > 20:
                patterns['detected_patterns'].append(f'{col}: Categorical/grouping column')
                patterns['pattern_details'][f'{col}_categorical'] = {
                    'unique_values': int(df[col].nunique()),
                    'pattern_type': 'categorical',
                    'top_values': df[col].value_counts().head(3).to_dict()
                }
        
        # Pattern 3: Colonnes num√©riques avec faible variance (constantes)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df[col].std() / df[col].mean() < 0.1 and df[col].mean() != 0:  # Coefficient de variation faible
                patterns['detected_patterns'].append(f'{col}: Low variance (nearly constant)')
                patterns['pattern_details'][f'{col}_low_variance'] = {
                    'coefficient_of_variation': round(df[col].std() / df[col].mean(), 4),
                    'pattern_type': 'low_variance'
                }
        
        # Pattern 4: Colonnes avec distribution bimodale
        for col in numeric_cols:
            if len(df[col].dropna()) > 10:
                # D√©tection simple de bimodalit√© bas√©e sur l'histogramme
                hist, _ = np.histogram(df[col].dropna(), bins=10)
                peaks = np.where((hist[1:-1] > hist[:-2]) & (hist[1:-1] > hist[2:]))[0] + 1
                if len(peaks) >= 2:
                    patterns['detected_patterns'].append(f'{col}: Potential bimodal distribution')
                    patterns['pattern_details'][f'{col}_bimodal'] = {
                        'peaks_detected': int(len(peaks)),
                        'pattern_type': 'bimodal_distribution'
                    }
        
        return patterns
    
    def _analyze_data_distributions(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse de la distribution des donn√©es"""
        
        distributions = {}
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            data = df[col].dropna()
            if len(data) > 5:
                distributions[col] = {
                    'distribution_type': self._classify_distribution(data),
                    'skewness': round(data.skew(), 3),
                    'kurtosis': round(data.kurtosis(), 3),
                    'normality_test': self._test_normality_simple(data),
                    'outliers_info': self._analyze_outliers_detailed(data)
                }
        
        return distributions
    
    def _classify_distribution(self, data: pd.Series) -> str:
        """Classification simple du type de distribution"""
        
        skewness = data.skew()
        kurtosis = data.kurtosis()
        
        if abs(skewness) < 0.5 and abs(kurtosis) < 3:
            return 'approximately_normal'
        elif skewness > 1:
            return 'right_skewed'
        elif skewness < -1:
            return 'left_skewed'
        elif kurtosis > 3:
            return 'heavy_tailed'
        elif kurtosis < -1:
            return 'light_tailed'
        else:
            return 'unknown'
    
    def _test_normality_simple(self, data: pd.Series) -> str:
        """Test de normalit√© simple bas√© sur les statistiques"""
        
        if len(data) < 8:
            return 'insufficient_data'
        
        skewness = abs(data.skew())
        kurtosis = abs(data.kurtosis())
        
        if skewness < 0.5 and kurtosis < 1:
            return 'likely_normal'
        elif skewness > 2 or kurtosis > 4:
            return 'clearly_non_normal'
        else:
            return 'possibly_normal'
    
    def _detect_anomalies_universal(self, df: pd.DataFrame) -> Dict[str, Any]:
        """D√©tection universelle d'anomalies"""
        
        anomalies = {
            'outliers': [],
            'data_quality_issues': [],
            'anomaly_summary': {}
        }
        
        # D√©tection d'outliers pour colonnes num√©riques
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        total_outliers = 0
        
        for col in numeric_cols:
            outliers = self._detect_outliers_iqr(df[col])
            if len(outliers) > 0:
                total_outliers += len(outliers)
                anomalies['outliers'].append({
                    'column': col,
                    'outlier_count': len(outliers),
                    'outlier_indices': outliers.tolist()[:10],  # Limiter √† 10 pour √©viter surcharge
                    'outlier_percentage': round(len(outliers) / len(df) * 100, 2)
                })
        
        # D√©tection de probl√®mes de qualit√©
        for col in df.columns:
            # Valeurs aberrantes dans les strings (longueurs inhabituelles)
            if df[col].dtype == 'object':
                sample = df[col].dropna().astype(str)
                if len(sample) > 0:
                    avg_length = sample.str.len().mean()
                    std_length = sample.str.len().std()
                    
                    unusual_lengths = sample[sample.str.len() > avg_length + 3 * std_length]
                    if len(unusual_lengths) > 0:
                        anomalies['data_quality_issues'].append({
                            'column': col,
                            'issue': 'unusual_string_lengths',
                            'count': len(unusual_lengths),
                            'examples': unusual_lengths.head(3).tolist()
                        })
            
            # Valeurs dupliqu√©es suspectes
            if df[col].dtype != 'object':  # Pour colonnes num√©riques
                if df[col].duplicated().sum() > len(df) * 0.5:  # Plus de 50% de doublons
                    anomalies['data_quality_issues'].append({
                        'column': col,
                        'issue': 'excessive_duplicates',
                        'duplicate_percentage': round(df[col].duplicated().sum() / len(df) * 100, 2)
                    })
        
        anomalies['anomaly_summary'] = {
            'total_outliers': total_outliers,
            'columns_with_outliers': len([x for x in anomalies['outliers']]),
            'data_quality_issues_count': len(anomalies['data_quality_issues'])
        }
        
        return anomalies
    
    def _detect_outliers_iqr(self, series: pd.Series) -> pd.Index:
        """D√©tection d'outliers avec m√©thode IQR"""
        
        data = series.dropna()
        if len(data) < 4:
            return pd.Index([])
        
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outlier_mask = (data < lower_bound) | (data > upper_bound)
        return data[outlier_mask].index
    
    def _count_outliers(self, data: pd.Series) -> int:
        """Compte les outliers dans une s√©rie"""
        return len(self._detect_outliers_iqr(data))
    
    def _analyze_outliers_detailed(self, data: pd.Series) -> Dict[str, Any]:
        """Analyse d√©taill√©e des outliers"""
        
        outliers_indices = self._detect_outliers_iqr(data)
        
        if len(outliers_indices) == 0:
            return {
                'outliers_count': 0,
                'outliers_percentage': 0,
                'has_outliers': False
            }
        
        outliers_values = data[outliers_indices]
        
        return {
            'outliers_count': len(outliers_indices),
            'outliers_percentage': round(len(outliers_indices) / len(data) * 100, 2),
            'has_outliers': True,
            'outliers_range': {
                'min': float(outliers_values.min()),
                'max': float(outliers_values.max())
            },
            'main_data_range': {
                'min': float(data.quantile(0.25) - 1.5 * (data.quantile(0.75) - data.quantile(0.25))),
                'max': float(data.quantile(0.75) + 1.5 * (data.quantile(0.75) - data.quantile(0.25)))
            }
        }
    
    def _summarize_metrics_analysis(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """R√©sum√© de l'analyse des m√©triques"""
        
        if not metrics:
            return {'message': 'Aucune m√©trique de performance identifi√©e'}
        
        summary = {
            'total_metrics': len(metrics),
            'metric_types': {},
            'key_insights': []
        }
        
        # Classifier les types de m√©triques
        for metric_name, metric_data in metrics.items():
            if 'percentage' in metric_name.lower() or '%' in metric_name:
                summary['metric_types']['percentage'] = summary['metric_types'].get('percentage', 0) + 1
            elif any(keyword in metric_name.lower() for keyword in ['price', 'cost', 'amount', 'revenue']):
                summary['metric_types']['financial'] = summary['metric_types'].get('financial', 0) + 1
            else:
                summary['metric_types']['other'] = summary['metric_types'].get('other', 0) + 1
        
        # G√©n√©rer des insights cl√©s
        if summary['metric_types'].get('percentage', 0) > 0:
            summary['key_insights'].append(f"{summary['metric_types']['percentage']} m√©triques en pourcentage d√©tect√©es")
        
        if summary['metric_types'].get('financial', 0) > 0:
            summary['key_insights'].append(f"{summary['metric_types']['financial']} m√©triques financi√®res identifi√©es")
        
        return summary
    
    def _generate_analysis_summary(self, df: pd.DataFrame, domain_detection: Dict[str, Any]) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© complet de l'analyse"""
        
        return {
            'dataset_characteristics': {
                'rows': len(df),
                'columns': len(df.columns),
                'estimated_domain': domain_detection.get('primary_domain', 'unknown'),
                'domain_confidence': domain_detection.get('confidence', 0)
            },
            'data_quality_overview': {
                'completeness': round((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100, 2),
                'columns_with_missing_data': int((df.isnull().sum() > 0).sum()),
                'duplicate_rows': int(df.duplicated().sum())
            },
            'column_type_breakdown': {
                'numeric': int(len(df.select_dtypes(include=[np.number]).columns)),
                'text': int(len(df.select_dtypes(include=['object']).columns)),
                'datetime': int(len(df.select_dtypes(include=['datetime64']).columns))
            },
            'analysis_recommendations': self._generate_analysis_recommendations(df, domain_detection)
        }
    
    def _generate_analysis_recommendations(self, df: pd.DataFrame, domain_detection: Dict[str, Any]) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur l'analyse"""
        
        recommendations = []
        
        # Recommandations bas√©es sur la qualit√© des donn√©es
        missing_data_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
        if missing_data_ratio > 0.1:
            recommendations.append(f"Am√©liorer la qualit√© des donn√©es - {missing_data_ratio:.1%} de valeurs manquantes")
        
        # Recommandations bas√©es sur le domaine
        domain = domain_detection.get('primary_domain', 'unknown')
        if domain == 'rental_car':
            recommendations.append("Analyser les patterns d'upselling par branche pour optimiser les revenus")
            recommendations.append("Identifier les facteurs de succ√®s des branches les plus performantes")
        elif domain == 'sales':
            recommendations.append("Analyser la performance des vendeurs pour identifier les meilleures pratiques")
            recommendations.append("Optimiser la r√©partition territoriale bas√©e sur les r√©sultats")
        elif domain == 'hr':
            recommendations.append("Analyser les patterns de r√©tention et de performance des employ√©s")
            recommendations.append("Identifier les d√©partements n√©cessitant plus d'attention")
        elif domain == 'unknown':
            recommendations.append("Analyser plus en d√©tail pour identifier le domaine m√©tier sp√©cifique")
        
        # Recommandations bas√©es sur la structure des donn√©es
        numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
        if numeric_cols > 3:
            recommendations.append("Effectuer une analyse de corr√©lation approfondie entre les m√©triques num√©riques")
        
        if len(df) > 1000:
            recommendations.append("Consid√©rer une segmentation des donn√©es pour une analyse plus granulaire")
        
        return recommendations[:5]  # Limiter √† 5 recommandations principales
    
    def _generate_business_insights(self, df: pd.DataFrame, domain_info: Dict[str, Any]) -> List[str]:
        """G√©n√®re des insights business automatiques"""
        
        insights = []
        domain = domain_info.get('primary_domain', 'unknown')
        
        # Insights g√©n√©raux sur la taille du dataset
        insights.append(f"Dataset de {len(df)} enregistrements analys√©s avec {len(df.columns)} variables")
        
        # Insights sp√©cifiques au domaine
        if domain == 'rental_car':
            # Chercher des patterns sp√©cifiques au rental car
            branch_col = self._find_column_containing(df, ['branch', 'location'])
            if branch_col:
                unique_branches = df[branch_col].nunique()
                insights.append(f"Analyse de {unique_branches} branches/locations diff√©rentes")
                
                # Top performer
                performance_cols = self._find_columns_containing(df, ['price', 'revenue', 'amount'])
                if performance_cols:
                    perf_col = performance_cols[0]
                    if df[perf_col].dtype in ['float64', 'int64']:
                        top_branch = df.groupby(branch_col)[perf_col].mean().idxmax()
                        insights.append(f"Branche la plus performante: {top_branch}")
            
            # Analyser les patterns d'upselling
            upsell_cols = self._find_columns_containing(df, ['upsell', 'upgrade', 'up'])
            if upsell_cols:
                insights.append(f"Donn√©es d'upselling disponibles pour {len(upsell_cols)} m√©triques")
        
        elif domain == 'sales':
            # Insights ventes
            revenue_cols = self._find_columns_containing(df, ['revenue', 'sales', 'amount'])
            if revenue_cols:
                total_revenue = df[revenue_cols[0]].sum() if df[revenue_cols[0]].dtype in ['float64', 'int64'] else 0
                insights.append(f"Revenus total analys√©: {total_revenue:,.2f}" if total_revenue > 0 else "Donn√©es de revenus d√©tect√©es")
        
        elif domain == 'hr':
            # Insights RH
            employee_col = self._find_column_containing(df, ['employee', 'staff'])
            if employee_col:
                total_employees = df[employee_col].nunique()
                insights.append(f"Donn√©es de {total_employees} employ√©s analys√©es")
        
        # Insights sur la qualit√© des donn√©es
        missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
        if missing_ratio < 0.05:
            insights.append("Excellente qualit√© des donn√©es (< 5% de valeurs manquantes)")
        elif missing_ratio > 0.2:
            insights.append(f"Qualit√© des donn√©es √† am√©liorer ({missing_ratio:.1%} de valeurs manquantes)")
        
        # Insights sur la diversit√© des donn√©es
        numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
        if numeric_cols > len(df.columns) / 2:
            insights.append("Dataset riche en m√©triques quantitatives - excellente base pour l'analyse")
        
        return insights[:7]  # Limiter √† 7 insights pour la lisibilit√©
    
    def _calculate_entropy(self, value_counts: pd.Series) -> float:
        """Calcule l'entropie d'une distribution"""
        
        if len(value_counts) == 0:
            return 0
        
        total = value_counts.sum()
        probabilities = value_counts / total
        entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
        
        return round(entropy, 3)
    
    def _calculate_concentration_ratio(self, value_counts: pd.Series) -> float:
        """Calcule le ratio de concentration (part des top valeurs)"""
        
        if len(value_counts) == 0:
            return 0
        
        total = value_counts.sum()
        top_values_sum = value_counts.head(min(3, len(value_counts))).sum()
        
        return round(top_values_sum / total, 3)
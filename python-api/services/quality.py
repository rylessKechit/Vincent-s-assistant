"""
Service de v√©rification de la qualit√© des donn√©es
Analyse de compl√©tude, coh√©rence et fiabilit√© des datasets
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from loguru import logger

class QualityChecker:
    """V√©rificateur de qualit√© des donn√©es"""
    
    def __init__(self):
        self.ready = True
        self.quality_thresholds = {
            'completeness': 0.8,  # 80% de donn√©es compl√®tes
            'consistency': 0.9,   # 90% de coh√©rence
            'validity': 0.85,     # 85% de validit√©
            'accuracy': 0.8       # 80% de pr√©cision estim√©e
        }
    
    def is_ready(self) -> bool:
        return self.ready
    
    def _get_dataframe(self, df_input: Any) -> pd.DataFrame:
        """Reconstituer le DataFrame depuis diff√©rents formats"""
        if isinstance(df_input, dict):
            if 'data' in df_input and 'columns' in df_input:
                return pd.DataFrame(df_input['data'])
            elif 'data' in df_input:
                return pd.DataFrame(df_input['data'])
            else:
                return pd.DataFrame(df_input)
        elif isinstance(df_input, pd.DataFrame):
            return df_input
        else:
            logger.warning(f"Format DataFrame inconnu pour quality check: {type(df_input)}")
            return pd.DataFrame()
    
    async def check_quality(self, df_input: Any, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        V√©rification compl√®te de la qualit√© des donn√©es
        
        Returns:
            Dict contenant tous les indicateurs de qualit√©
        """
        try:
            df = self._get_dataframe(df_input)
            
            if df.empty:
                return self._get_empty_quality_report()
            
            quality_report = {
                'overall_score': 0.0,
                'completeness': self._check_completeness(df),
                'consistency': self._check_consistency(df),
                'validity': self._check_validity(df),
                'accuracy': self._check_accuracy(df),
                'uniqueness': self._check_uniqueness(df),
                'timeliness': self._check_timeliness(df, metadata),
                'integrity': self._check_integrity(df),
                'anomalies': self._detect_anomalies(df),
                'recommendations': []
            }
            
            # Calculer le score global
            quality_report['overall_score'] = self._calculate_overall_score(quality_report)
            
            # G√©n√©rer des recommandations
            quality_report['recommendations'] = self._generate_quality_recommendations(quality_report)
            
            logger.info(f"Quality check termin√© - Score: {quality_report['overall_score']:.1f}%")
            return quality_report
            
        except Exception as e:
            logger.error(f"Erreur quality check: {str(e)}")
            return self._get_error_quality_report(str(e))
    
    def _get_empty_quality_report(self) -> Dict[str, Any]:
        """Rapport de qualit√© pour un DataFrame vide"""
        return {
            'overall_score': 0.0,
            'completeness': {'score': 0.0, 'issues': ['Dataset vide']},
            'consistency': {'score': 0.0, 'issues': ['Dataset vide']},
            'validity': {'score': 0.0, 'issues': ['Dataset vide']},
            'accuracy': {'score': 0.0, 'issues': ['Dataset vide']},
            'uniqueness': {'score': 0.0, 'issues': ['Dataset vide']},
            'timeliness': {'score': 0.0, 'issues': ['Dataset vide']},
            'integrity': {'score': 0.0, 'issues': ['Dataset vide']},
            'anomalies': [],
            'recommendations': ['V√©rifier la source des donn√©es']
        }
    
    def _get_error_quality_report(self, error_msg: str) -> Dict[str, Any]:
        """Rapport de qualit√© en cas d'erreur"""
        return {
            'overall_score': 0.0,
            'completeness': {'score': 0.0, 'issues': [f'Erreur: {error_msg}']},
            'consistency': {'score': 0.0, 'issues': []},
            'validity': {'score': 0.0, 'issues': []},
            'accuracy': {'score': 0.0, 'issues': []},
            'uniqueness': {'score': 0.0, 'issues': []},
            'timeliness': {'score': 0.0, 'issues': []},
            'integrity': {'score': 0.0, 'issues': []},
            'anomalies': [],
            'recommendations': ['V√©rifier le format des donn√©es']
        }
    
    def _check_completeness(self, df: pd.DataFrame) -> Dict[str, Any]:
        """V√©rification de la compl√©tude des donn√©es"""
        total_cells = df.shape[0] * df.shape[1]
        missing_cells = df.isnull().sum().sum()
        completeness_ratio = (total_cells - missing_cells) / total_cells if total_cells > 0 else 0
        
        issues = []
        column_completeness = {}
        
        for col in df.columns:
            col_completeness = 1 - (df[col].isnull().sum() / len(df))
            column_completeness[col] = float(col_completeness)
            
            if col_completeness < 0.8:
                issues.append(f"Colonne '{col}': {(1-col_completeness)*100:.1f}% de valeurs manquantes")
        
        if completeness_ratio < self.quality_thresholds['completeness']:
            issues.append(f"Compl√©tude globale faible: {completeness_ratio*100:.1f}%")
        
        return {
            'score': float(completeness_ratio * 100),
            'missing_cells': int(missing_cells),
            'total_cells': int(total_cells),
            'column_completeness': column_completeness,
            'issues': issues
        }
    
    def _check_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """V√©rification de la coh√©rence des donn√©es"""
        issues = []
        consistency_score = 1.0
        
        # 1. V√©rifier la coh√©rence des types par colonne
        type_inconsistencies = 0
        for col in df.columns:
            if df[col].dtype == 'object':
                # V√©rifier si certaines valeurs ressemblent √† des nombres
                sample_values = df[col].dropna().head(100)
                numeric_like = 0
                for val in sample_values:
                    try:
                        float(str(val).replace(',', '').replace('"', ''))
                        numeric_like += 1
                    except:
                        pass
                
                if 0.3 < numeric_like / len(sample_values) < 0.9:
                    issues.append(f"Colonne '{col}': Types mixtes d√©tect√©s")
                    type_inconsistencies += 1
        
        # 2. V√©rifier les formats de donn√©es
        for col in df.columns:
            col_lower = col.lower()
            
            # V√©rifier les formats de pourcentage
            if '%' in col_lower or 'rate' in col_lower:
                sample_values = df[col].astype(str).head(50)
                has_percent_symbol = sample_values.str.contains('%').sum()
                if 0 < has_percent_symbol < len(sample_values) * 0.8:
                    issues.append(f"Colonne '{col}': Format pourcentage incoh√©rent")
            
            # V√©rifier les formats mon√©taires
            if any(keyword in col_lower for keyword in ['revenue', 'price', 'cost', 'package']):
                sample_values = df[col].astype(str).head(50)
                has_comma = sample_values.str.contains(',').sum()
                has_quotes = sample_values.str.contains('"').sum()
                if has_comma > 0 and has_quotes != has_comma:
                    issues.append(f"Colonne '{col}': Format mon√©taire incoh√©rent")
        
        # 3. V√©rifier les patterns d'agents SIXT
        agent_cols = [col for col in df.columns if 'agent' in col.lower()]
        if agent_cols:
            agent_col = agent_cols[0]
            agent_values = df[agent_col].astype(str)
            
            # Pattern attendu: ID - Nom ou Exit Employee
            valid_pattern_count = 0
            for val in agent_values.head(100):
                if ' - ' in val or 'exit employee' in val.lower():
                    valid_pattern_count += 1
            
            pattern_ratio = valid_pattern_count / min(100, len(agent_values))
            if pattern_ratio < 0.9:
                issues.append(f"Colonne '{agent_col}': Pattern agent incoh√©rent")
        
        # Calculer le score de coh√©rence
        if type_inconsistencies > 0:
            consistency_score -= (type_inconsistencies / len(df.columns)) * 0.3
        
        consistency_score = max(0, consistency_score)
        
        return {
            'score': float(consistency_score * 100),
            'type_inconsistencies': type_inconsistencies,
            'issues': issues
        }
    
    def _check_validity(self, df: pd.DataFrame) -> Dict[str, Any]:
        """V√©rification de la validit√© des donn√©es"""
        issues = []
        validity_score = 1.0
        invalid_values_count = 0
        
        for col in df.columns:
            col_lower = col.lower()
            
            # V√©rifier les valeurs num√©riques n√©gatives inappropri√©es
            if any(keyword in col_lower for keyword in ['count', 'number', '#', 'contracts']):
                try:
                    # Nettoyer et convertir en num√©rique
                    cleaned_series = df[col].astype(str).str.replace(',', '').str.replace('"', '')
                    numeric_series = pd.to_numeric(cleaned_series, errors='coerce')
                    negative_count = (numeric_series < 0).sum()
                    
                    if negative_count > 0:
                        issues.append(f"Colonne '{col}': {negative_count} valeurs n√©gatives inappropri√©es")
                        invalid_values_count += negative_count
                except:
                    pass
            
            # V√©rifier les pourcentages hors limites
            if '%' in col or 'rate' in col_lower or 'share' in col_lower:
                try:
                    # Nettoyer les pourcentages
                    cleaned_series = df[col].astype(str).str.replace('%', '').str.replace(',', '')
                    numeric_series = pd.to_numeric(cleaned_series, errors='coerce')
                    
                    # V√©rifier si les valeurs sont dans une plage raisonnable
                    if not numeric_series.isna().all():
                        out_of_range = ((numeric_series < 0) | (numeric_series > 100)).sum()
                        if out_of_range > 0:
                            issues.append(f"Colonne '{col}': {out_of_range} pourcentages hors limites")
                            invalid_values_count += out_of_range
                except:
                    pass
            
            # V√©rifier les valeurs aberrantes pour les colonnes financi√®res
            if any(keyword in col_lower for keyword in ['revenue', 'package', 'ir']):
                try:
                    cleaned_series = df[col].astype(str).str.replace(',', '').str.replace('"', '')
                    numeric_series = pd.to_numeric(cleaned_series, errors='coerce')
                    
                    if not numeric_series.isna().all():
                        # Valeurs excessivement grandes (plus de 1M)
                        excessive_values = (numeric_series > 1000000).sum()
                        if excessive_values > 0:
                            issues.append(f"Colonne '{col}': {excessive_values} valeurs potentiellement aberrantes")
                except:
                    pass
        
        # Calculer le score de validit√©
        total_data_points = df.shape[0] * df.shape[1]
        if total_data_points > 0:
            validity_ratio = 1 - (invalid_values_count / total_data_points)
            validity_score = max(0, validity_ratio)
        
        return {
            'score': float(validity_score * 100),
            'invalid_values_count': int(invalid_values_count),
            'issues': issues
        }
    
    def _check_accuracy(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Estimation de la pr√©cision des donn√©es"""
        issues = []
        accuracy_indicators = []
        
        # 1. V√©rifier la coh√©rence des calculs (si possible)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        # 2. V√©rifier les totaux ou sous-totaux
        for col in df.columns:
            col_lower = col.lower()
            if 'total' in col_lower or 'sum' in col_lower:
                # Chercher une ligne de total
                last_rows = df.tail(3)
                for idx, row in last_rows.iterrows():
                    if any(keyword in str(row.iloc[0]).lower() for keyword in ['total', 'sum', ',']):
                        accuracy_indicators.append("Ligne de total d√©tect√©e")
                        break
        
        # 3. V√©rifier la coh√©rence des agents Exit Employee
        agent_cols = [col for col in df.columns if 'agent' in col.lower()]
        if agent_cols and len(numeric_cols) > 0:
            agent_col = agent_cols[0]
            exit_mask = df[agent_col].astype(str).str.contains('Exit Employee', na=False, case=False)
            
            if exit_mask.sum() > 0:
                # V√©rifier si les Exit Employees ont des donn√©es coh√©rentes (souvent des z√©ros)
                exit_data = df[exit_mask]
                for num_col in numeric_cols[:3]:  # V√©rifier les 3 premi√®res colonnes num√©riques
                    try:
                        cleaned_series = exit_data[num_col].astype(str).str.replace(',', '').str.replace('"', '')
                        numeric_series = pd.to_numeric(cleaned_series, errors='coerce')
                        zero_ratio = (numeric_series == 0).sum() / len(numeric_series) if len(numeric_series) > 0 else 0
                        
                        if zero_ratio > 0.8:
                            accuracy_indicators.append(f"Exit Employees avec donn√©es coh√©rentes (mostly zeros) in {num_col}")
                    except:
                        pass
        
        # 4. V√©rifier les patterns de donn√©es
        # Rechercher des patterns r√©p√©titifs suspects
        for col in df.columns:
            if df[col].dtype in ['object']:
                value_counts = df[col].value_counts()
                if len(value_counts) > 0:
                    most_common_ratio = value_counts.iloc[0] / len(df)
                    if most_common_ratio > 0.5 and len(value_counts) > 1:
                        issues.append(f"Colonne '{col}': {most_common_ratio*100:.1f}% de valeurs identiques")
        
        # Score bas√© sur les indicateurs trouv√©s
        base_accuracy = 80.0  # Score de base
        
        # Bonus pour chaque indicateur positif
        accuracy_bonus = len(accuracy_indicators) * 5
        
        # Malus pour chaque probl√®me
        accuracy_malus = len(issues) * 10
        
        final_accuracy = max(0, min(100, base_accuracy + accuracy_bonus - accuracy_malus))
        
        return {
            'score': float(final_accuracy),
            'indicators': accuracy_indicators,
            'issues': issues
        }
    
    def _check_uniqueness(self, df: pd.DataFrame) -> Dict[str, Any]:
        """V√©rification de l'unicit√© des donn√©es"""
        issues = []
        
        # V√©rifier les doublons de lignes
        duplicate_rows = df.duplicated().sum()
        
        # V√©rifier l'unicit√© des identifiants
        id_columns = []
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['id', 'agent', 'employee']):
                id_columns.append(col)
        
        duplicate_ids = 0
        for col in id_columns:
            col_duplicates = df[col].duplicated().sum()
            duplicate_ids += col_duplicates
            if col_duplicates > 0:
                issues.append(f"Colonne '{col}': {col_duplicates} valeurs dupliqu√©es")
        
        # Calculer le score d'unicit√©
        total_expected_unique = len(df) * len(id_columns) if id_columns else len(df)
        total_duplicates = duplicate_rows + duplicate_ids
        
        uniqueness_score = 1 - (total_duplicates / max(1, total_expected_unique))
        uniqueness_score = max(0, uniqueness_score)
        
        if duplicate_rows > 0:
            issues.append(f"{duplicate_rows} lignes enti√®rement dupliqu√©es")
        
        return {
            'score': float(uniqueness_score * 100),
            'duplicate_rows': int(duplicate_rows),
            'duplicate_ids': int(duplicate_ids),
            'issues': issues
        }
    
    def _check_timeliness(self, df: pd.DataFrame, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """V√©rification de la fra√Æcheur des donn√©es"""
        issues = []
        timeliness_score = 85.0  # Score par d√©faut
        
        # Chercher des colonnes de dates
        date_columns = []
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['date', 'month', 'year', 'time']):
                date_columns.append(col)
        
        if date_columns:
            for col in date_columns:
                try:
                    # Tenter de parser les dates
                    date_series = pd.to_datetime(df[col], errors='coerce')
                    valid_dates = date_series.dropna()
                    
                    if len(valid_dates) > 0:
                        latest_date = valid_dates.max()
                        oldest_date = valid_dates.min()
                        
                        # V√©rifier si les donn√©es sont r√©centes (derni√®re ann√©e)
                        import datetime
                        now = datetime.datetime.now()
                        days_since_latest = (now - latest_date).days if pd.notna(latest_date) else 9999
                        
                        if days_since_latest > 365:
                            issues.append(f"Colonne '{col}': Donn√©es datent de plus d'un an")
                            timeliness_score -= 20
                        elif days_since_latest > 30:
                            issues.append(f"Colonne '{col}': Donn√©es datent de plus d'un mois")
                            timeliness_score -= 10
                        
                        # V√©rifier la plage de dates
                        date_range = (latest_date - oldest_date).days if pd.notna(latest_date) and pd.notna(oldest_date) else 0
                        if date_range > 365 * 2:  # Plus de 2 ans
                            issues.append(f"Colonne '{col}': Tr√®s large plage temporelle ({date_range} jours)")
                        
                except Exception as e:
                    logger.debug(f"Erreur parsing dates pour {col}: {e}")
        else:
            # Pas de colonnes de dates d√©tect√©es
            issues.append("Aucune colonne de date d√©tect√©e - impossible de v√©rifier la fra√Æcheur")
            timeliness_score = 60.0
        
        return {
            'score': float(max(0, timeliness_score)),
            'date_columns_found': len(date_columns),
            'issues': issues
        }
    
    def _check_integrity(self, df: pd.DataFrame) -> Dict[str, Any]:
        """V√©rification de l'int√©grit√© structurelle"""
        issues = []
        integrity_score = 100.0
        
        # 1. V√©rifier la coh√©rence des en-t√™tes
        empty_headers = [col for col in df.columns if col.strip() == '' or pd.isna(col)]
        if empty_headers:
            issues.append(f"{len(empty_headers)} colonnes sans nom")
            integrity_score -= 20
        
        # 2. V√©rifier les colonnes enti√®rement vides
        empty_columns = [col for col in df.columns if df[col].isna().all()]
        if empty_columns:
            issues.append(f"{len(empty_columns)} colonnes enti√®rement vides: {empty_columns}")
            integrity_score -= 10 * len(empty_columns)
        
        # 3. V√©rifier les lignes enti√®rement vides
        empty_rows = df.isna().all(axis=1).sum()
        if empty_rows > 0:
            issues.append(f"{empty_rows} lignes enti√®rement vides")
            integrity_score -= min(20, empty_rows * 2)
        
        # 4. V√©rifier la coh√©rence du nombre de colonnes
        if len(df.columns) < 2:
            issues.append("Tr√®s peu de colonnes d√©tect√©es")
            integrity_score -= 30
        
        # 5. V√©rifier les caract√®res sp√©ciaux probl√©matiques
        special_char_issues = 0
        for col in df.columns:
            if any(char in col for char in ['\n', '\r', '\t']):
                special_char_issues += 1
        
        if special_char_issues > 0:
            issues.append(f"{special_char_issues} colonnes avec caract√®res de contr√¥le")
            integrity_score -= special_char_issues * 5
        
        integrity_score = max(0, integrity_score)
        
        return {
            'score': float(integrity_score),
            'empty_columns': len(empty_columns),
            'empty_rows': int(empty_rows),
            'special_char_issues': special_char_issues,
            'issues': issues
        }
    
    def _detect_anomalies(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """D√©tection d'anomalies dans les donn√©es"""
        anomalies = []
        
        # 1. Anomalies num√©riques (outliers extr√™mes)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            series = df[col].dropna()
            if len(series) > 4:
                Q1 = series.quantile(0.25)
                Q3 = series.quantile(0.75)
                IQR = Q3 - Q1
                extreme_outliers = series[(series < Q1 - 3*IQR) | (series > Q3 + 3*IQR)]
                
                if len(extreme_outliers) > 0:
                    anomalies.append({
                        'type': 'extreme_outlier',
                        'column': col,
                        'count': int(len(extreme_outliers)),
                        'description': f"{len(extreme_outliers)} valeurs extr√™mement aberrantes"
                    })
        
        # 2. Anomalies de format
        for col in df.columns:
            if df[col].dtype == 'object':
                # D√©tecter des formats mixtes inhabituels
                sample_values = df[col].dropna().astype(str).head(100)
                lengths = sample_values.str.len()
                
                if len(lengths) > 10:
                    length_std = lengths.std()
                    length_mean = lengths.mean()
                    
                    # Si tr√®s grande variation de longueur
                    if length_std > length_mean * 0.5:
                        anomalies.append({
                            'type': 'format_inconsistency',
                            'column': col,
                            'count': 1,
                            'description': f"Grandes variations de longueur (std: {length_std:.1f})"
                        })
        
        # 3. Anomalies m√©tier (sp√©cifiques SIXT)
        agent_cols = [col for col in df.columns if 'agent' in col.lower()]
        if agent_cols:
            agent_col = agent_cols[0]
            
            # V√©rifier si des agents ont des noms suspects
            agent_values = df[agent_col].astype(str)
            suspicious_agents = agent_values[agent_values.str.contains(r'[0-9]{10,}', na=False)]
            
            if len(suspicious_agents) > 0:
                anomalies.append({
                    'type': 'data_quality',
                    'column': agent_col,
                    'count': int(len(suspicious_agents)),
                    'description': "Agents avec noms suspects (IDs tr√®s longs)"
                })
        
        return anomalies
    
    def _calculate_overall_score(self, quality_report: Dict[str, Any]) -> float:
        """Calcul du score global de qualit√©"""
        weights = {
            'completeness': 0.25,
            'consistency': 0.20,
            'validity': 0.20,
            'accuracy': 0.15,
            'uniqueness': 0.10,
            'timeliness': 0.05,
            'integrity': 0.05
        }
        
        weighted_score = 0.0
        total_weight = 0.0
        
        for dimension, weight in weights.items():
            if dimension in quality_report and 'score' in quality_report[dimension]:
                weighted_score += quality_report[dimension]['score'] * weight
                total_weight += weight
        
        return weighted_score / total_weight if total_weight > 0 else 0.0
    
    def _generate_quality_recommendations(self, quality_report: Dict[str, Any]) -> List[str]:
        """G√©n√©ration de recommandations bas√©es sur l'analyse qualit√©"""
        recommendations = []
        overall_score = quality_report.get('overall_score', 0)
        
        # Recommandations g√©n√©rales bas√©es sur le score
        if overall_score >= 90:
            recommendations.append("üéâ Excellente qualit√© de donn√©es - dataset pr√™t pour l'analyse")
        elif overall_score >= 80:
            recommendations.append("‚úÖ Bonne qualit√© de donn√©es - quelques am√©liorations mineures possibles")
        elif overall_score >= 70:
            recommendations.append("‚ö†Ô∏è Qualit√© acceptable - attention aux points de vigilance")
        elif overall_score >= 60:
            recommendations.append("‚ùå Qualit√© probl√©matique - nettoyage recommand√©")
        else:
            recommendations.append("üö® Qualit√© critique - r√©vision compl√®te n√©cessaire")
        
        # Recommandations sp√©cifiques par dimension
        if quality_report.get('completeness', {}).get('score', 100) < 80:
            recommendations.append("üìä Traiter les valeurs manquantes avant analyse")
        
        if quality_report.get('consistency', {}).get('score', 100) < 80:
            recommendations.append("üîß Standardiser les formats de donn√©es")
        
        if quality_report.get('validity', {}).get('score', 100) < 80:
            recommendations.append("‚úÇÔ∏è Nettoyer les valeurs aberrantes")
        
        if quality_report.get('uniqueness', {}).get('score', 100) < 90:
            recommendations.append("üóëÔ∏è √âliminer les doublons d√©tect√©s")
        
        if quality_report.get('timeliness', {}).get('score', 100) < 70:
            recommendations.append("üìÖ V√©rifier la fra√Æcheur des donn√©es")
        
        # Recommandations bas√©es sur les anomalies
        anomalies = quality_report.get('anomalies', [])
        if anomalies:
            recommendations.append(f"üîç Investiguer {len(anomalies)} anomalie(s) d√©tect√©e(s)")
        
        return recommendations
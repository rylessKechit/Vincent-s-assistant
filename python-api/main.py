"""
API Python pour l'analyse de donn√©es intelligente
Version RENFORC√âE avec validation stricte - Compatible 100% avec l'existant
"""

import os
import sys
import time
from typing import Dict, Any, List, Union, Optional
from contextlib import asynccontextmanager

# Configuration des logs en premier
from loguru import logger
logger.remove()
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logger.add(sys.stdout, level=log_level, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")

# Imports FastAPI
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# Imports des services
from services.extractor import CSVExtractor
from services.analyzer import DataAnalyzer
from services.classifier import QueryClassifier
from services.quality import QualityChecker

# Models Pydantic pour validation (IDENTIQUES √† l'existant)
class QueryRequest(BaseModel):
    question: str
    available_columns: List[str]
    context: Dict[str, Any] = {}

class AggregationRequest(BaseModel):
    question: str
    dataframe_data: Dict[str, Any]
    aggregation_type: str = "smart"

class ExtractionResponse(BaseModel):
    success: bool
    data: Dict[str, Any]
    processing_time_ms: float
    error: Optional[str] = None

class ClassificationResponse(BaseModel):
    type: str
    confidence: float
    relevant_columns: List[str]
    suggested_strategy: str
    processing_time_ms: float

class AggregationResponse(BaseModel):
    success: bool
    aggregations: Dict[str, Any]
    processing_time_ms: float
    error: Optional[str] = None

# Initialisation des services
extractor = CSVExtractor()
analyzer = DataAnalyzer()
classifier = QueryClassifier()
quality_checker = QualityChecker()

# Fonction de validation renforc√©e
def validate_extraction_result(extraction_result: Dict[str, Any], step: str) -> None:
    """Validation stricte des r√©sultats d'extraction"""
    if not extraction_result:
        raise ValueError(f"R√©sultat d'extraction vide √† l'√©tape: {step}")
    
    if not extraction_result.get('success', False):
        error_msg = extraction_result.get('error', 'Erreur inconnue')
        raise ValueError(f"√âchec extraction √† l'√©tape {step}: {error_msg}")
    
    if 'dataframe_data' not in extraction_result:
        raise ValueError(f"dataframe_data manquant √† l'√©tape: {step}")
    
    dataframe_data = extraction_result['dataframe_data']
    if not dataframe_data or not isinstance(dataframe_data, dict):
        raise ValueError(f"dataframe_data invalide √† l'√©tape: {step}")
    
    if not dataframe_data.get('data') or not dataframe_data.get('columns'):
        raise ValueError(f"Donn√©es DataFrame incompl√®tes √† l'√©tape: {step}")
    
    logger.success(f"‚úÖ Validation r√©ussie: {step}")

def validate_analysis_result(analysis_result: Dict[str, Any], step: str) -> None:
    """Validation des r√©sultats d'analyse"""
    if not analysis_result or not isinstance(analysis_result, dict):
        raise ValueError(f"R√©sultat d'analyse invalide √† l'√©tape: {step}")
    
    logger.success(f"‚úÖ Validation analyse r√©ussie: {step}")

# Lifecycle management
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("üöÄ D√©marrage de l'API Python RENFORC√âE...")
    
    try:
        # V√©rification des services
        if not extractor.is_ready():
            raise Exception("Service Extractor non pr√™t")
        
        if not analyzer.is_ready():
            raise Exception("Service Analyzer non pr√™t")
        
        if not quality_checker.is_ready():
            raise Exception("Service QualityChecker non pr√™t")
        
        # Initialisation des mod√®les
        await classifier.load_models()
        
        logger.success("‚úÖ API Python RENFORC√âE pr√™te et op√©rationnelle!")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur initialisation: {str(e)}")
        raise
    
    yield
    
    logger.info("üõë Arr√™t de l'API Python")

# Application FastAPI
app = FastAPI(
    title="AI-Assistant Python API RENFORC√âE",
    description="API d'analyse de donn√©es intelligente avec ML - Version robuste",
    version="1.1.0",
    lifespan=lifespan
)

# Configuration CORS (identique)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifier les domaines autoris√©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    """V√©rification de la sant√© de l'API - RENFORC√âE"""
    try:
        services_status = {
            "extractor": extractor.is_ready(),
            "analyzer": analyzer.is_ready(),
            "classifier": classifier.is_ready(),
            "quality_checker": quality_checker.is_ready()
        }
        
        all_healthy = all(services_status.values())
        
        return {
            "status": "healthy" if all_healthy else "degraded",
            "timestamp": time.time(),
            "services": services_status,
            "version": "1.1.0-renforc√©e"
        }
        
    except Exception as e:
        logger.error(f"Erreur health check: {str(e)}")
        return {
            "status": "unhealthy",
            "timestamp": time.time(),
            "error": str(e)
        }

@app.post("/extract", response_model=ExtractionResponse)
async def extract_and_analyze(file: UploadFile = File(...)):
    """
    Extraction et analyse compl√®te d'un fichier CSV - VERSION RENFORC√âE
    Compatible 100% avec l'existant mais avec validation stricte
    """
    start_time = time.time()
    
    try:
        # √âTAPE 1: Validation du fichier (renforc√©e)
        logger.info(f"üì• Validation du fichier: {file.filename}")
        
        if not file.filename:
            raise HTTPException(status_code=400, detail="Nom de fichier manquant")
        
        if not file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="Seuls les fichiers CSV sont support√©s")
        
        if file.size is None:
            raise HTTPException(status_code=400, detail="Taille de fichier ind√©termin√©e")
        
        if file.size == 0:
            raise HTTPException(status_code=400, detail="Fichier vide")
            
        if file.size > 50 * 1024 * 1024:  # 50MB
            raise HTTPException(status_code=400, detail="Fichier trop volumineux (max 50MB)")
        
        # √âTAPE 2: Lecture du contenu (avec validation)
        logger.info(f"üìñ Lecture du contenu: {file.filename} ({file.size} bytes)")
        
        content = await file.read()
        if not content:
            raise HTTPException(status_code=400, detail="Contenu de fichier vide apr√®s lecture")
        
        logger.info(f"‚úÖ Contenu lu: {len(content)} bytes")
        
        # √âTAPE 3: Extraction CSV (avec validation stricte)
        logger.info("üìä D√©but extraction CSV...")
        
        extraction_result = await extractor.extract_csv(content, file.filename)
        
        # VALIDATION STRICTE de l'extraction
        validate_extraction_result(extraction_result, "Extraction CSV")
        
        dataframe_data = extraction_result['dataframe_data']
        metadata = extraction_result.get('metadata', {})
        
        logger.info(f"‚úÖ Extraction r√©ussie: {dataframe_data['shape']['rows']} lignes √ó {dataframe_data['shape']['columns']} colonnes")
        
        # √âTAPE 4: Analyse des donn√©es (avec validation)
        logger.info("üß† D√©but analyse des donn√©es...")
        
        analysis_result = await analyzer.analyze_dataframe(dataframe_data, metadata)
        validate_analysis_result(analysis_result, "Analyse donn√©es")
        
        # √âTAPE 5: D√©tection des patterns m√©tier (avec validation)
        logger.info("üéØ D√©tection patterns m√©tier...")
        
        business_patterns = await analyzer.detect_business_patterns(dataframe_data)
        validate_analysis_result(business_patterns, "D√©tection patterns")
        
        # √âTAPE 6: V√©rification de la qualit√© (avec validation)
        logger.info("‚úÖ V√©rification qualit√©...")
        
        quality_result = await quality_checker.check_quality(dataframe_data, metadata)
        validate_analysis_result(quality_result, "V√©rification qualit√©")
        
        # √âTAPE 7: G√©n√©ration des recommandations (avec validation)
        logger.info("üí° G√©n√©ration recommandations...")
        
        recommendations = await analyzer.generate_recommendations(dataframe_data, business_patterns)
        if not isinstance(recommendations, list):
            logger.warning("Recommandations non-liste, conversion en liste")
            recommendations = [str(recommendations)] if recommendations else []
        
        processing_time = (time.time() - start_time) * 1000
        
        # √âTAPE 8: Construction de la r√©ponse finale (avec validation JSON)
        logger.info("üì¶ Construction r√©ponse finale...")
        
        response_data = {
            "extraction": extraction_result,
            "analysis": analysis_result,
            "quality": quality_result,
            "business_patterns": business_patterns,
            "recommendations": recommendations
        }
        
        # VALIDATION FINALE: V√©rifier que la r√©ponse est s√©rialisable JSON
        try:
            import json
            json.dumps(response_data)
            logger.success("‚úÖ Validation JSON r√©ussie")
        except Exception as json_error:
            logger.error(f"‚ùå Erreur s√©rialisation JSON: {str(json_error)}")
            raise HTTPException(status_code=500, detail="Erreur s√©rialisation des r√©sultats")
        
        logger.success(f"üéâ Analyse compl√®te termin√©e en {processing_time:.0f}ms")
        
        return ExtractionResponse(
            success=True,
            data=response_data,
            processing_time_ms=processing_time,
            error=None
        )
        
    except HTTPException:
        # Re-lever les HTTPException (erreurs client)
        raise
    
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        error_msg = f"Erreur lors du traitement: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        
        return ExtractionResponse(
            success=False,
            data={},
            processing_time_ms=processing_time,
            error=error_msg
        )

@app.post("/classify", response_model=ClassificationResponse)
async def classify_question(request: QueryRequest):
    """
    Classification intelligente d'une question - COMPATIBLE
    """
    start_time = time.time()
    
    try:
        logger.info(f"üß† Classification question: {request.question[:50]}...")
        
        result = await classifier.classify_question(
            request.question,
            request.available_columns,
            request.context
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        logger.success(f"‚úÖ Classification r√©ussie: {result['type']} (confidence: {result['confidence']:.2f})")
        
        return ClassificationResponse(
            type=result['type'],
            confidence=result['confidence'],
            relevant_columns=result['relevant_columns'],
            suggested_strategy=result['suggested_strategy'],
            processing_time_ms=processing_time
        )
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Erreur classification: {str(e)}")
        
        # Fallback classification (comme avant)
        return ClassificationResponse(
            type="semantic",
            confidence=0.5,
            relevant_columns=[],
            suggested_strategy="Classification par d√©faut (erreur)",
            processing_time_ms=processing_time
        )

@app.post("/aggregate", response_model=AggregationResponse)
async def compute_aggregations(request: AggregationRequest):
    """
    Calcul d'agr√©gations intelligentes - COMPATIBLE
    """
    start_time = time.time()
    
    try:
        logger.info(f"üìä Calcul agr√©gations pour: {request.question[:50]}...")
        
        aggregations = await analyzer.compute_smart_aggregations(
            request.dataframe_data,
            request.question,
            request.aggregation_type
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        logger.success(f"‚úÖ Agr√©gations calcul√©es en {processing_time:.0f}ms")
        
        return AggregationResponse(
            success=True,
            aggregations=aggregations,
            processing_time_ms=processing_time,
            error=None
        )
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        error_msg = f"Erreur agr√©gations: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        
        return AggregationResponse(
            success=False,
            aggregations={},
            processing_time_ms=processing_time,
            error=error_msg
        )

@app.get("/status")
async def get_status():
    """Statut d√©taill√© de l'API - RENFORC√â"""
    try:
        uptime = time.time()
        
        # V√©rification rapide des services
        services_health = {
            "extractor": extractor.is_ready(),
            "analyzer": analyzer.is_ready(),
            "classifier": classifier.is_ready(),
            "quality_checker": quality_checker.is_ready()
        }
        
        return {
            "status": "running",
            "version": "1.1.0-renforc√©e",
            "uptime": uptime,
            "services_health": services_health,
            "endpoints": {
                "/health": "V√©rification sant√© renforc√©e",
                "/extract": "Extraction et analyse CSV avec validation stricte",
                "/classify": "Classification de questions",
                "/aggregate": "Calculs d'agr√©gations"
            }
        }
        
    except Exception as e:
        logger.error(f"Erreur status: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

# Gestion des erreurs globales (RENFORC√âE)
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"‚ùå Erreur non g√©r√©e: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": "Erreur interne du serveur",
            "detail": str(exc),
            "timestamp": time.time()
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )